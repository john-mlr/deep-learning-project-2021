{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "import h5py\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import cm\n",
    "from PIL import Image\n",
    "import openslide\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Kmeans analysis of BYOL derived latent space\n",
    "In this notebook I will perform kmeans clustering on feature vectors produced by BYOL. The dataset is AT8 immunohistochemically stained whole slide images of human hippocampus that have been digitized. The kmeans clustering will show how richely the features represent the anatomy and pathology of the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is a Pytorch dataloader which yields a tensor of a 256x256 patch of the provided slide image at the index of coordinates. The function uses an h5file which is the ouput of a segmentation function provided by CLAM (https://github.com/mahmoodlab/CLAM/blob/master/create_patches.py) which is an array of top-left coordinates of 256x256 patches that contain tissue. This ensures that we do not get any background in our analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureCreationDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" This dataset assumes that only one slide will be instantiated with.\n",
    "        Additionally, it assumes no transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, slide='', h5file='', tilesize = 256):\n",
    "        self.tilesize = tilesize\n",
    "        \n",
    "        self.slidepath = slide\n",
    "        self.wsi = openslide.OpenSlide(slide)\n",
    "        \n",
    "        self.h5path = h5file\n",
    "        f = h5py.File(h5file,'r')\n",
    "        self.coords = np.asarray(f['coords'])\n",
    "\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return(self.coords.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        coord = self.coords[idx,:]\n",
    "        tile = self.wsi.read_region((coord[0], coord[1]),0,(self.tilesize, self.tilesize)).convert('RGB')\n",
    "\n",
    "        # img = img_as_float32(img)\n",
    "        img = self.transform(tile)\n",
    "            \n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will create the feature vectors based on the supplied slide images using a supplied pretrained BYOL model. It can also create feature vectors using Resnet50 pretrained on an Imagenet classification task (weights provided by Pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_byol_features_to_np(slide='', h5file='', checkpoint=\"/sc/arion/projects/tauomics/BYOL-tau-scoring/BYOL/models/trained_resnet6.pth\", imagenet_pretrained=False, tilesize = 256, batch_size = 400):\n",
    "    \"\"\" Make features from images by passing them through a resnet encoder\"\"\"\n",
    "    \n",
    "    tile_dataset = FeatureCreationDataset(slide=slide, h5file=h5file, tilesize = 256)\n",
    "    loader = torch.utils.data.DataLoader(tile_dataset,\n",
    "                                              batch_size=batch_size, \n",
    "                                              num_workers=5, \n",
    "                                              shuffle=False,\n",
    "                                              drop_last=False)\n",
    "\n",
    "    device=torch.device('cuda')\n",
    "    \n",
    "    # encoder setup\n",
    "    if imagenet_pretrained:\n",
    "        resnet = torchvision.models.resnet50(pretrained=True)\n",
    "        resnet.fc = nn.Identity()\n",
    "    else:\n",
    "        resnet = torchvision.models.resnet50(pretrained=False)  \n",
    "        resnet.fc = nn.Identity()\n",
    "        resnet.load_state_dict(torch.load(checkpoint))\n",
    "\n",
    "\n",
    "    resnet = nn.DataParallel(resnet)\n",
    "    resnet = resnet.to(device)\n",
    "    print(\"Model created\", flush=True)\n",
    "\n",
    "    feats = []\n",
    "    labels = []\n",
    "    resnet = resnet.eval()\n",
    "    for batch, (img) in enumerate(loader):\n",
    "        img = img.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            h_i = resnet(img)\n",
    "\n",
    "        \n",
    "        feats.append(h_i)\n",
    "    \n",
    "    feats = torch.cat(feats, dim=0)\n",
    "    feats = feats.squeeze()\n",
    "    feats = feats.cpu().numpy()\n",
    "    \n",
    "    print(\"Finished embedding. Feature array shape:\", feats.shape)    \n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes feature vectors and will run k-means clustering with a given k value and will return the cluster labels for each feature vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kmeans(feats, k=5, seed=51):\n",
    "    sample_kmeans = KMeans(n_clusters=k, random_state=seed)\n",
    "    labels = sample_kmeans.fit_predict(feats)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates maps based on the given k-cluster assignments of each patch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_maps(labels, h5file, w, h, outfile, downsample = 20, tiledim = 256):\n",
    "    \n",
    "    f = h5py.File(h5file,'r')\n",
    "    coords = np.asarray(f['coords'])\n",
    "\n",
    "\n",
    "    mask = np.zeros((h,w,3))\n",
    "\n",
    "    values = [ np.array(cm.Paired_r(i)[:3]) * 255 for i in set(labels) ]\n",
    "    cmap = dict(zip(set(labels),values))\n",
    "\n",
    "    for l,(x,y) in zip(labels,coords):\n",
    "        mask[y:y+tiledim,x:x+tiledim,:] = cmap[l]\n",
    "\n",
    "    image = Image.fromarray(mask.astype(np.uint8))\n",
    "    size = image.size\n",
    "    image = image.resize((int(size[0]/downsample),int(size[1]/downsample)))\n",
    "    image.save(outfile)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I do an example run through with a given case at k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "downsample = 70\n",
    "h5file = '/sc/arion/projects/tauomics/PART_images/Hippocampus_AT8_stain/clam/256_allslides/patches/45505.h5'\n",
    "slidepath = '/sc/arion/projects/tauomics/PART_images/Hippocampus_AT8_stain/45505.svs'\n",
    "outfile = 'k5_byol_45505.png'\n",
    "feats = make_features_to_np(slidepath, h5file, 256)\n",
    "labels = run_kmeans(feats, k)\n",
    "\n",
    "wsi = openslide.OpenSlide(slidepath)\n",
    "w,h = wsi.dimensions\n",
    "\n",
    "make_maps(labels,h5file,w,h,outfile, downsample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the resulting map: \n",
    "![k5_byol_45505](https://github.com/john-mlr/deep-learning-project-2021/blob/main/k5_byol_45505.png?raw=true)\n",
    "\n",
    "And the original slide image: \n",
    "![orig_at8_45505](https://github.com/john-mlr/deep-learning-project-2021/blob/main/orig_at8_45505.png?raw=true)\n",
    "\n",
    "We can see how the clusters conform to the anatomical features and pathology. \n",
    "\n",
    "At k = 2, we can see that tau is a major discriminator in the feature space: \n",
    "![k2_byol_45505](https://github.com/john-mlr/deep-learning-project-2021/blob/main/k2_byol_45505.png?raw=true)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is K-means clustering on the same slide using feature vectors from a Resnet50 pretrained with Imagenet.\n",
    "\n",
    "At k = 2:\n",
    "![k2_pretrained_45505](https://github.com/john-mlr/deep-learning-project-2021/blob/main/k2_pretrained_45505.png?raw=true)\n",
    "\n",
    "and at k = 5:\n",
    "![k5_pretrained_45505](https://github.com/john-mlr/deep-learning-project-2021/blob/main/k5_pretrained_45505.png?raw=true)\n",
    "\n",
    "While the feature space captured by Imagenet pretraining does an adequate job, it is not as fine grained and relevant to the pathology or anatomy as BYOL is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example with another slide. One with less tau burden. We find that the absence of tau results in more anatomy dependent features being brought out in the k-means clustering: \n",
    "\n",
    "Here is the original slide:\n",
    "![orig_at8_45883](https://github.com/john-mlr/deep-learning-project-2021/blob/main/orig_at8_45883.png?raw=true)\n",
    "Here is the clustering of BYOL features: \n",
    "BYOL at k = 2:\n",
    "![k2_byol_45883](https://github.com/john-mlr/deep-learning-project-2021/blob/main/k2_byol_45883.png?raw=true)\n",
    "BYOL at k = 5:\n",
    "![k5_byol_45883](https://github.com/john-mlr/deep-learning-project-2021/blob/main/k5_byol_45883.png?raw=true)\n",
    "Here is the clustering of Resnet50 pretrained with Imagenet:\n",
    "pretrained at k = 2:\n",
    "![k2_pretrained_45883](https://github.com/john-mlr/deep-learning-project-2021/blob/main/k2_pretrained_45883.png?raw=true)\n",
    "pretrained at k = 5:\n",
    "![k5_pretrained_45883](https://github.com/john-mlr/deep-learning-project-2021/blob/main/k5_pretrained_45883.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
